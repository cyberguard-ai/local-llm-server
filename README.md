# local-llm-server
A containerized FastAPI server for running open source Large Language Models locally using llama.cpp
